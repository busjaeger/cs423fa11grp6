2. Case Study 2
	For case study 2, we increased the memory used by each work process to 200 MB, so that in the scenario where N = 11, those 11 processes would use up 2.2 GB of memory, which is more than the 2 GB our VM has. In the cases where no major page faults occurred (N = 1, N = 5) is due to the fact that these workloads did not exceed the 2 GB memory limit of our VM. Similar to Observation 1 in Case Study 1. 
	For the case where N = 1 and N = 5, the same observation can be made as in Case Study 1, Observation 2. At the beginning, the minor page fault count is high and then tapers off. The same explanation for Observation 2 applies here as well.
	For the case of N = 1 and N = 5, both cases took about the same amount of time to run, case N = 1 had 403 profiled reads and the case N = 5 had 412 profiled reads. The difference between these cases is due to the number of work processes that were run. Increasing the number of work processes from 1 to 5, the graphs show an overall increase in CPU utilization in case N = 5. Running more processes concurrently while staying within the memory limits results in a better use of system resources. For the specific data that our group gathered, the total cpu utilization for N = 1 was 2.8 where the total cpu utilization for N = 5 was 15.36, which is a ~450% increase.
	In the case of N = 11, cpu utilization starts out consistently high but then changes as major page faults occur due to the 2 GB of available memory being taken up by these 11 processes each trying to use 200 MB. Thrashing causes this workload to take much longer to complete (1918 profiled reads) as these processes become disk-bound and compete for memory, constantly awaiting pages to be paged in that they need that other processes most likely evicted because they needed more pages brought in. Also, because this workload took longer to complete, it used more cpu time than the case N = 5. With our specific data, overall, total cpu utilization increased by ~2000%. 
	What can be concluded from this case study is that increasing the degree of multiprogramming (from N = 1 to N = 5) is desirable in that overall CPU utilization increases, but that there is a point where increasing that degree (to N = 11) results in thrashing which is undesirable. 
	
---------------------------------------------------------------------
	
	Case Study 2: Multiprogramming

observation:
why does cpu utilization spike at the end?



For case study 2, we increased the memory used by each work process to 200 MB, so that in the scenario where N = 11, those 11 processes would use up 2.2 GB of memory, which is more than the 2 GB our VM has. In the cases where no major page faults occured is due to the fact that these workloads did not exceed the 2 GB memory limit of our VM.

N = 1: Utilization starts out at ~40% and gradually drops by 10% and then by 5% and lower, to then vary between ~4% and ~8% before spiking back up to ~30% at the very end of the process' lifetime. No major faults occur, only minor.

N = 5: Utilization starts out at about ~90% and stays high for a little while before dropping 20% gradually over time before hovering between ~5% to ~20%, and then spiking again at the end to ~75%. No major faults occur, only minor. 

N = 11: utilization starts out consistently high in the 90s, then falls very low before major page faults start to occur and then we see the cpu utilization constantly go up and down as thrashing occurs (the processes that are running require too much memory and pages need to be paged in and out for the different processes). Thrashing caused this workload to take much longer to complete, where in the cases N = 1 and N = 5, 403 and 412 profiled reads were taken, respectively, in the case of N = 11, it recorded 1918 profiled reads, about 4 times the length to run to completion.

